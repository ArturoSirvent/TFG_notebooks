{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Analisis_CNN_2_gamma_others.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZdZ_-SlVkpiD-POXzR65sullTPCMsfwW",
      "authorship_tag": "ABX9TyMOFjYQXh69kWnAAPC88PRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArturoSirvent/TFG_notebooks/blob/main/single_input/Analisis_CNN_2_gamma_others.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh-bhDXqNF31"
      },
      "source": [
        "#cargamos librerias \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import os\n",
        "import tensorflow as tf\n",
        "import glob \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrPZ4jh_RHne"
      },
      "source": [
        "import matplotlib as mplt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiHbKIsbNdOJ"
      },
      "source": [
        "Vamos a automaticar el preoceso de :\n",
        "* Crear un modelo\n",
        "* Cargar los datos necesarios \n",
        "* Formatear los datos bien\n",
        "* Pasarle los datos a la red \n",
        "* compilarla y entrenarla \n",
        "* finalmente guardar los progresos asi como las funciones los y acuracy\n",
        "\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "Solo vamos a tomar una cantidad mas o menos equivalente de datos para cada entrenamiento, porque sino pera esto\n",
        "\n",
        "(Es posible que tenga problemas con los de masximo peso)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQAVP0I5Tgjk"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDdYiUGsTnyW"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqRU0rgdTnrL"
      },
      "source": [
        "tf.keras.models.Sequential?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CqGUbVcNNRp"
      },
      "source": [
        "#usamos la funcion crear modelo que ha usado Rrodigo en su tfg\n",
        " \n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        " \n",
        "def create_model(padding='same',stride=2,pool=2,filters=64,kernel=5,hidden_layers=2,neurons=20,optimizer='Adadelta',\n",
        "                 learn_rate=0.001, init_mode='glorot_normal', activation='relu', dropout_rate=0.4,\n",
        "                 weight_constraint=None,weight_regularizer=None, batchn=0,conv=2, n_filters=2,numClasses=2):#parameters here are default\n",
        " \n",
        "    # initialize the model\n",
        "    model = Sequential()                                                                       \n",
        " \n",
        "    for i in range (conv):\n",
        "        num=int(round(filters*(2**(i-1))))\n",
        "        for i in range (n_filters):\n",
        "              model.add(Conv2D(num, kernel, padding=padding,activation=activation,kernel_initializer=init_mode,kernel_constraint=weight_constraint,kernel_regularizer=weight_regularizer))\n",
        "        model.add(MaxPooling2D(pool_size=(pool, pool), strides=(stride, stride)))\n",
        "        # model.add(Dropout(dropout_rate))\n",
        "    # define the first FC => ACTIVATION layers\n",
        "    model.add(Flatten())\n",
        "    if (batchn==1):\n",
        "        model.add(BatchNormalization())     \n",
        "    model.add(Dropout(dropout_rate)) \n",
        "    for i in range(hidden_layers):\n",
        "        model.add(Dense(neurons,kernel_initializer=init_mode,  activation=activation,kernel_regularizer=weight_regularizer)) \n",
        "        if (batchn==1):\n",
        "            model.add(BatchNormalization())    #, use_bias=False\n",
        "        model.add(Dropout(dropout_rate)) \n",
        "    # lastly, define the soft-max classifier\n",
        "    model.add(Dense(numClasses, activation='softmax'))\n",
        " \n",
        "    if (optimizer=='SGD'):\n",
        "        optimizer = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    elif (optimizer=='RMS'):\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "    elif (optimizer=='Adagrad'):\n",
        "        optimizer=keras.optimizers.Adagrad(learning_rate=0.01)\n",
        "    elif (optimizer=='Adadelta'):\n",
        "        optimizer=keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95)\n",
        "    elif (optimizer=='Adamax'):\n",
        "        optimizer=keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
        "    elif (optimizer=='Nadam'):\n",
        "        optimizer=keras.optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
        "    elif (optimizer=='Adam'):\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        " \n",
        "    model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])   #ponemos esto en lugar de solo accuraci porque nos da un error al cargar el modelo de nuevo y hacer. Evaluate()           \n",
        " \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3dnWGrgXNNHE",
        "outputId": "9b3cf2ac-c03a-4c17-f2d2-7b78671042e3"
      },
      "source": [
        "npy_dir=\"/content/drive/MyDrive/TFG arturo/npy archivos\"\n",
        "direct_hist=\"/content/drive/MyDrive/TFG arturo/stats_1/clasif_CNN_1/\"\n",
        "#lo mejor va a ser tener cargadas unas cuantas de gamma y darselo conforme lo requiera \n",
        " \n",
        "nombre_aux=\"gamma_\"+str(1)+\".npy\"\n",
        "datos_gamma=np.load(os.path.join(npy_dir,nombre_aux))\n",
        "tama√±o_un_gamma=datos_gamma.nbytes #os.stat(\"/content/drive/MyDrive/TFG arturo/npy archivos/gamma_1.npy\").st_size\n",
        "elementos=[\"helium\"]\n",
        " \n",
        "for j in range(1,6):\n",
        "  nombre_aux=\"gamma_\"+str(j)+\".npy\"\n",
        "  load_aux=np.load(os.path.join(npy_dir,nombre_aux))\n",
        "  datos_gamma=np.concatenate((datos_gamma,load_aux),axis=0)\n",
        " \n",
        " \n",
        "for i in elementos:\n",
        "  #primero cargamos los datos necesarios\n",
        " \n",
        "  datos_elemento=np.load(os.path.join(npy_dir,i+\".npy\"))\n",
        "  \n",
        "  len_ds=int(round(datos_elemento.shape[0]/2))\n",
        "  datos_elemento=datos_elemento[:len_ds]\n",
        "  #vamos a cargar de datos los mismo pero de gammas\n",
        "  datos_gamma_aux=datos_gamma[:len_ds].copy()\n",
        "  datos=np.concatenate((datos_elemento,datos_gamma_aux),axis=0)\n",
        " \n",
        "  labels0=np.zeros(len_ds)\n",
        "  labels1=np.ones(len_ds)\n",
        "  labels=np.concatenate((labels0,labels1),axis=0)\n",
        "  X_train_aux,X_test_aux,y_train,y_test=train_test_split(datos,labels,test_size=0.20, random_state=3)\n",
        "  X_train=np.expand_dims(X_train_aux,axis=-1)\n",
        "  X_test=np.expand_dims(X_test_aux,axis=-1)\n",
        "  data_train=tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(128)\n",
        "  data_train=data_train.prefetch(1)\n",
        "  data_test=tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(128)\n",
        "  data_test=data_test.prefetch(1) \n",
        "  shape=[128,55,93,1]\n",
        "  model=create_model()\n",
        "  model.build(shape) \n",
        "  history=model.fit(data_train, epochs=60,batch_size=128, validation_data=data_test)  \n",
        "  \n",
        "  #vamos a borrar lo que no necesitamos\n",
        "  del data_train, datos_elemento, datos, datos_gamma_aux\n",
        "  del X_train, X_train_aux, y_train\n",
        "  model.save(direct_hist+\"modelo_\"+i+\".h5\")\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss, gamma vs.'+i)\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper right')\n",
        "  plt.savefig(direct_hist+\"loss_train_gamma_vs_\"+i+\".png\")\n",
        "  plt.close()\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "  plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "  plt.title('Model accuracy, gamma vs.'+i)\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='lower right')\n",
        "  plt.savefig(direct_hist+\"acc_train_gamma_vs_\"+i+\".png\")\n",
        "  plt.close()\n",
        " \n",
        "  val_loss, val_acc = model.evaluate(data_test)\n",
        "  y_pred = model.predict_classes(X_test)\n",
        "  conf_aux=confusion_matrix(y_test, y_pred)\n",
        "  report_aux=classification_report(y_test, y_pred)\n",
        "  with open(direct_hist+i+\"_report.txt\",\"w\") as report:\n",
        "    report.write(\"val_loss: \"+str(val_loss)+\"\\t\"+\"val_acc: \"+str(val_acc)+\"\\n\")\n",
        "    report.write(str(conf_aux)+\"\\n\"+str(report_aux)+\"\\n\")\n",
        "    report.close()\n",
        " \n",
        "  plt.figure(figsize=(15,15))\n",
        "  y_percent=model.predict(X_test)\n",
        "  y_percent=y_percent[:,0][y_test!=y_pred]\n",
        "  y_pred_mal=y_pred[y_test!=y_pred]\n",
        "  errores=X_test_aux[y_test!=y_pred]\n",
        "  if (errores.shape[0]>=16):\n",
        "    rango=range(1,17)\n",
        "    asd=4 \n",
        "    asd2=4\n",
        "    abortar=False\n",
        " \n",
        "  elif errores.shape[0]>=9:\n",
        "    rango=range(1,10)\n",
        "    asd=3\n",
        "    asd2=3\n",
        "    abortar=False\n",
        " \n",
        "  elif errores.shape[0]>=4:\n",
        "    rango=range(1,5)\n",
        "    asd=2\n",
        "    asd2=2\n",
        "    abortar=False\n",
        "  elif errores.shape[0]>=2:\n",
        "    rango=range(1,3)\n",
        "    asd=2     \n",
        "    asd2=1\n",
        "    abortar=False\n",
        "  elif errores.shape[0]>=1:\n",
        "    rango=range(1,2)\n",
        "    asd=1\n",
        "    asd2=1\n",
        "    abortar=False \n",
        "  else:\n",
        "    abortar=True \n",
        "  if not abortar:\n",
        "    for k in rango:\n",
        "      plt.subplot(asd,asd2,k)\n",
        "      num_mal=y_pred_mal[k-1]\n",
        "      if num_mal==1 :\n",
        "        nombre_bien=i\n",
        "        nombre_mal=\"gamma\"\n",
        "        prob=round((1-y_percent[k-1])*100,4)\n",
        "      elif num_mal==0:\n",
        "        nombre_bien=\"gamma\"\n",
        "        nombre_mal=i\n",
        "        prob=round((y_percent[k-1])*100,4)\n",
        "      plt.title(\"Es un \"+nombre_bien+\" pero \\n ha dicho con un \"+str(prob)+\"% \\n\"+\"que era un \"+nombre_mal, fontsize=14)\n",
        "      plt.imshow(errores[k-1])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(direct_hist+\"errores_\"+i+\".png\")\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "WARNING:tensorflow:Layer conv2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "  2/124 [..............................] - ETA: 3s - loss: 0.6931 - sparse_categorical_accuracy: 0.5156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0204s vs `on_train_batch_end` time: 0.0353s). Check your callbacks.\n",
            "124/124 [==============================] - 8s 67ms/step - loss: 0.4109 - sparse_categorical_accuracy: 0.8080 - val_loss: 0.0597 - val_sparse_categorical_accuracy: 0.9801\n",
            "Epoch 2/60\n",
            "124/124 [==============================] - 8s 62ms/step - loss: 0.0619 - sparse_categorical_accuracy: 0.9867 - val_loss: 0.0028 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 3/60\n",
            "124/124 [==============================] - 8s 63ms/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.0036 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 4/60\n",
            "124/124 [==============================] - 8s 63ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9972 - val_loss: 0.0025 - val_sparse_categorical_accuracy: 0.9992\n",
            "Epoch 5/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.0016 - val_sparse_categorical_accuracy: 0.9992\n",
            "Epoch 6/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9979 - val_loss: 6.9897e-04 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 7/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9967 - val_loss: 0.0018 - val_sparse_categorical_accuracy: 0.9987\n",
            "Epoch 8/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9940 - val_loss: 6.2452e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 9/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9980 - val_loss: 5.5553e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 10/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9984 - val_loss: 5.2862e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 11/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9952 - val_loss: 5.2039e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 12/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.0010 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 13/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9981 - val_loss: 5.6585e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 14/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9967 - val_loss: 8.6754e-04 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 15/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9979 - val_loss: 5.5646e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 16/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9978 - val_loss: 4.8984e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 17/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9979 - val_loss: 4.9479e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 18/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9985 - val_loss: 5.8192e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 19/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9960 - val_loss: 8.4431e-04 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 20/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9985 - val_loss: 5.1248e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 21/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9982 - val_loss: 5.2175e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 22/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0054 - sparse_categorical_accuracy: 0.9983 - val_loss: 5.3043e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 23/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0054 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0022 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 24/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9982 - val_loss: 5.5977e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 25/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9944 - val_loss: 7.5725e-04 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 26/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9984 - val_loss: 4.9146e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 27/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9989 - val_loss: 5.0211e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 28/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9985 - val_loss: 5.2505e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 29/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9983 - val_loss: 5.0760e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 30/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.0042 - val_sparse_categorical_accuracy: 0.9987\n",
            "Epoch 31/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0085 - sparse_categorical_accuracy: 0.9984 - val_loss: 5.0757e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 32/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9983 - val_loss: 5.4413e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 33/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0038 - sparse_categorical_accuracy: 0.9986 - val_loss: 5.2705e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 34/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0085 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.0097 - val_sparse_categorical_accuracy: 0.9987\n",
            "Epoch 35/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9984 - val_loss: 5.9645e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 36/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9989 - val_loss: 4.8446e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 37/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9980 - val_loss: 5.2766e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 38/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9987 - val_loss: 4.9620e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 39/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9984 - val_loss: 4.8345e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 40/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9989 - val_loss: 0.0369 - val_sparse_categorical_accuracy: 0.9987\n",
            "Epoch 41/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9987 - val_loss: 6.6302e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 42/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9989 - val_loss: 5.3994e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 43/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0029 - sparse_categorical_accuracy: 0.9989 - val_loss: 6.3113e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 44/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9987 - val_loss: 6.1610e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 45/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9987 - val_loss: 6.3953e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 46/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0027 - sparse_categorical_accuracy: 0.9990 - val_loss: 5.8904e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 47/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9980 - val_loss: 4.8428e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 48/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9984 - val_loss: 6.1671e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 49/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9988 - val_loss: 4.8945e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 50/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0047 - sparse_categorical_accuracy: 0.9984 - val_loss: 4.9843e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 51/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9987 - val_loss: 5.6492e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 52/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0042 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0039 - val_sparse_categorical_accuracy: 0.9992\n",
            "Epoch 53/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9987 - val_loss: 8.2857e-04 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 54/60\n",
            "124/124 [==============================] - 8s 65ms/step - loss: 0.0026 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.0013 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 55/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.0306 - val_sparse_categorical_accuracy: 0.9987\n",
            "Epoch 56/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9987 - val_loss: 5.0303e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 57/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0043 - sparse_categorical_accuracy: 0.9987 - val_loss: 6.2661e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 58/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0052 - val_sparse_categorical_accuracy: 0.9995\n",
            "Epoch 59/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9983 - val_loss: 6.0511e-04 - val_sparse_categorical_accuracy: 0.9997\n",
            "Epoch 60/60\n",
            "124/124 [==============================] - 8s 64ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9992 - val_loss: 7.7094e-04 - val_sparse_categorical_accuracy: 0.9997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b2f7d79c5f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy, gamma vs.'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq5A7BG6ZMej",
        "outputId": "78d5277d-9786-4143-9e56-bcfb36b85ed7"
      },
      "source": [
        "val_loss, val_acc = model.evaluate(data_test)\n",
        "y_pred = model.predict_classes(X_test)\n",
        "conf_aux=confusion_matrix(y_test, y_pred)\n",
        "report_aux=classification_report(y_test, y_pred)\n",
        "with open(direct_hist+i+\"_report.txt\",\"w\") as report:\n",
        "  report.write(\"val_loss: \"+str(val_loss)+\"\\t\"+\"val_acc: \"+str(val_acc)+\"\\n\")\n",
        "  report.write(str(conf_aux)+\"\\n\"+str(report_aux)+\"\\n\")\n",
        "  report.close()\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "y_percent=model.predict(X_test)\n",
        "y_percent=y_percent[:,0][y_test!=y_pred]\n",
        "y_pred_mal=y_pred[y_test!=y_pred]\n",
        "errores=X_test_aux[y_test!=y_pred]\n",
        "if (errores.shape[0]>=16):\n",
        "  rango=range(1,17)\n",
        "  asd=4 \n",
        "  asd2=4\n",
        "  abortar=False\n",
        "\n",
        "elif errores.shape[0]>=9:\n",
        "  rango=range(1,10)\n",
        "  asd=3\n",
        "  asd2=3\n",
        "  abortar=False\n",
        "\n",
        "elif errores.shape[0]>=4:\n",
        "  rango=range(1,5)\n",
        "  asd=2\n",
        "  asd2=2\n",
        "  abortar=False\n",
        "elif errores.shape[0]>=2:\n",
        "  rango=range(1,3)\n",
        "  asd=2     \n",
        "  asd2=1\n",
        "  abortar=False\n",
        "elif errores.shape[0]>=1:\n",
        "  rango=range(1,2)\n",
        "  asd=1\n",
        "  asd2=1\n",
        "  abortar=False \n",
        "else:\n",
        "  abortar=True \n",
        "if not abortar:\n",
        "  for k in rango:\n",
        "    plt.subplot(asd,asd2,k)\n",
        "    num_mal=y_pred_mal[k-1]\n",
        "    if num_mal==1 :\n",
        "      nombre_bien=i\n",
        "      nombre_mal=\"gamma\"\n",
        "      prob=round((1-y_percent[k-1])*100,4)\n",
        "    elif num_mal==0:\n",
        "      nombre_bien=\"gamma\"\n",
        "      nombre_mal=i\n",
        "      prob=round((y_percent[k-1])*100,4)\n",
        "    plt.title(\"Es un \"+nombre_bien+\" pero \\n ha dicho con un \"+str(prob)+\"% \\n\"+\"que era un \"+nombre_mal, fontsize=14)\n",
        "    plt.imshow(errores[k-1])\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(direct_hist+\"errores_\"+i+\".png\")\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 1s 19ms/step - loss: 7.7094e-04 - sparse_categorical_accuracy: 0.9997\n",
            "WARNING:tensorflow:From <ipython-input-7-daa6e93dc737>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hpXr4en7Kwl"
      },
      "source": [
        " \n",
        "direct_hist=\"/content/drive/MyDrive/TFG arturo/stats_1/clasif_CNN_1/\"\n",
        "mod=tf.keras.models.load_model(direct_hist+\"modelo_nitrogen.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "LVh6OEgh8zRg",
        "outputId": "054804a6-d095-4c47-db11-481d8a508b4e"
      },
      "source": [
        " \n",
        "npy_dir=\"/content/drive/MyDrive/TFG arturo/npy archivos\"\n",
        "direct_hist=\"/content/drive/MyDrive/TFG arturo/stats_1/clasif_CNN_1/\"\n",
        "#lo mejor va a ser tener cargadas unas cuantas de gamma y darselo conforme lo requiera \n",
        " \n",
        "nombre_aux=\"gamma_\"+str(1)+\".npy\"\n",
        "datos_gamma=np.load(os.path.join(npy_dir,nombre_aux))\n",
        "tama√±o_un_gamma=datos_gamma.nbytes #os.stat(\"/content/drive/MyDrive/TFG arturo/npy archivos/gamma_1.npy\").st_size\n",
        "elementos=[\"nitrogen\"]#[\"helium\",\"silicon\",\"iron\",\"electron\",\"proton\",\"nitrogen\"]\n",
        " \n",
        "for j in range(1,6):\n",
        "  nombre_aux=\"gamma_\"+str(j)+\".npy\"\n",
        "  load_aux=np.load(os.path.join(npy_dir,nombre_aux))\n",
        "  datos_gamma=np.concatenate((datos_gamma,load_aux),axis=0)\n",
        " \n",
        " \n",
        "for i in elementos:\n",
        "  #primero cargamos los datos necesarios\n",
        " \n",
        "  datos_elemento=np.load(os.path.join(npy_dir,i+\".npy\"))\n",
        "  len_ds=datos_elemento.shape[0]\n",
        "  #vamos a cargar de datos los mismo pero de gammas\n",
        "  datos_gamma_aux=datos_gamma[:len_ds].copy()\n",
        "  datos=np.concatenate((datos_elemento,datos_gamma_aux),axis=0)\n",
        " \n",
        "  labels0=np.zeros(len_ds)\n",
        "  labels1=np.ones(len_ds)\n",
        "  labels=np.concatenate((labels0,labels1),axis=0)\n",
        "  X_train_aux,X_test_aux,y_train,y_test=train_test_split(datos,labels,test_size=0.20, random_state=3)\n",
        "  #X_train=np.expand_dims(X_train_aux,axis=-1)\n",
        "  X_test=np.expand_dims(X_test_aux,axis=-1)\n",
        "  #data_train=tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(128)\n",
        "  #data_train=data_train.prefetch(1)\n",
        "  data_test=tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(128)\n",
        "  data_test=data_test.prefetch(1) \n",
        "  shape=[128,55,93,1]\n",
        "  #model=create_model()\n",
        "  #model.build(shape) \n",
        "  #history=model.fit(data_train, epochs=60,batch_size=128, validation_data=data_test)  \n",
        "  model=tf.keras.models.load_model(direct_hist+\"modelo_\"+i+\".h5\")\n",
        "  #model.save(direct_hist+\"modelo_\"+i+\".h5\")\n",
        "  \n",
        "  print(i)\n",
        "  val_loss, val_acc = model.evaluate(data_test)\n",
        "  print(str(val_acc))\n",
        "  val_loss, val_acc =model.evaluate(data_test)\n",
        "  print(str(val_acc))\n",
        "  \n",
        "  y_pred = model.predict_classes(X_test)\n",
        "  conf_aux=confusion_matrix(y_test, y_pred)\n",
        "  a=y_test.shape[0]\n",
        "  b=y_test[y_test != y_pred]\n",
        "  print(\"test_acc bien: \" +str(1-b.shape[0]/a))\n",
        "  print(conf_aux)\n",
        "  report_aux=classification_report(y_test, y_pred)\n",
        "  with open(direct_hist+i+\"_report2.txt\",\"w\") as report:\n",
        "    report.write(i)\n",
        "    report.write(str(val_acc))\n",
        "    report.write(\"test_acc bien: \" +str(1-b.shape[0]/a))\n",
        "    report.write(str(conf_aux))\n",
        "    report.write(str(conf_aux)+\"\\n\"+str(report_aux)+\"\\n\")\n",
        "    report.close()\n",
        "\"\"\"\n",
        "with open(direct_hist+i+\"_report.txt\",\"w\") as report:\n",
        "    report.write(\"val_loss: \"+str(val_loss)+\"\\t\"+\"val_acc: \"+str(val_acc)+\"\\n\")\n",
        "    report.write(str(conf_aux)+\"\\n\"+str(report_aux)+\"\\n\")\n",
        "    report.close()\n",
        " \n",
        "  plt.figure(figsize=(15,15))\n",
        "  y_percent=model.predict(X_test)\n",
        "  y_percent=y_percent[:,0][y_test!=y_pred]\n",
        "  y_pred_mal=y_pred[y_test!=y_pred]\n",
        "  errores=X_test_aux[y_test!=y_pred]\n",
        "  if (errores.shape[0]>=16):\n",
        "    rango=range(1,17)\n",
        "    asd=4 \n",
        "    asd2=4\n",
        "    abortar=False\n",
        " \n",
        "  elif errores.shape[0]>=9:\n",
        "    rango=range(1,10)\n",
        "    asd=3\n",
        "    asd2=3\n",
        "    abortar=False\n",
        " \n",
        "  elif errores.shape[0]>=4:\n",
        "    rango=range(1,5)\n",
        "    asd=2\n",
        "    asd2=2\n",
        "    abortar=False\n",
        "  elif errores.shape[0]>=2:\n",
        "    rango=range(1,3)\n",
        "    asd=2     \n",
        "    asd2=1\n",
        "    abortar=False\n",
        "  elif errores.shape[0]>=1:\n",
        "    rango=range(1,2)\n",
        "    asd=1\n",
        "    asd2=1\n",
        "    abortar=False \n",
        "  else:\n",
        "    abortar=True\n",
        " \n",
        "  if not abortar:\n",
        "    for k in rango:\n",
        "      plt.subplot(asd,asd2,k)\n",
        "      num_mal=y_pred_mal[k-1]\n",
        "      if num_mal==1 :\n",
        "        nombre_bien=i\n",
        "        nombre_mal=\"gamma\"\n",
        "        prob=round((1-y_percent[k-1])*100,4)\n",
        "      elif num_mal==0:\n",
        "        nombre_bien=\"gamma\"\n",
        "        nombre_mal=i\n",
        "        prob=round((y_percent[k-1])*100,4)\n",
        "      plt.title(\"Es un \"+nombre_bien+\" pero \\n ha dicho con un \"+str(prob)+\"% \\n\"+\"que era un \"+nombre_mal, fontsize=14)\n",
        "      plt.imshow(errores[k-1])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(direct_hist+\"errores_\"+i+\".png\")\n",
        "    plt.close()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nitrogen\n",
            "35/35 [==============================] - 1s 18ms/step - loss: 0.0130 - accuracy: 0.5029\n",
            "0.5028532147407532\n",
            "35/35 [==============================] - 1s 16ms/step - loss: 0.0130 - accuracy: 0.5029\n",
            "0.5028532147407532\n",
            "WARNING:tensorflow:From <ipython-input-4-871b43174811>:49: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "test_acc bien: 0.9993152248345126\n",
            "[[2201    1]\n",
            " [   2 2177]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nwith open(direct_hist+i+\"_report.txt\",\"w\") as report:\\n   report.write(\"val_loss: \"+str(val_loss)+\"\\t\"+\"val_acc: \"+str(val_acc)+\"\\n\")\\n   report.write(str(conf_aux)+\"\\n\"+str(report_aux)+\"\\n\")\\n   report.close()\\n\\n plt.figure(figsize=(15,15))\\n y_percent=model.predict(X_test)\\n y_percent=y_percent[:,0][y_test!=y_pred]\\n y_pred_mal=y_pred[y_test!=y_pred]\\n errores=X_test_aux[y_test!=y_pred]\\n if (errores.shape[0]>=16):\\n   rango=range(1,17)\\n   asd=4 \\n   asd2=4\\n   abortar=False\\n\\n elif errores.shape[0]>=9:\\n   rango=range(1,10)\\n   asd=3\\n   asd2=3\\n   abortar=False\\n\\n elif errores.shape[0]>=4:\\n   rango=range(1,5)\\n   asd=2\\n   asd2=2\\n   abortar=False\\n elif errores.shape[0]>=2:\\n   rango=range(1,3)\\n   asd=2     \\n   asd2=1\\n   abortar=False\\n elif errores.shape[0]>=1:\\n   rango=range(1,2)\\n   asd=1\\n   asd2=1\\n   abortar=False \\n else:\\n   abortar=True\\n\\n if not abortar:\\n   for k in rango:\\n     plt.subplot(asd,asd2,k)\\n     num_mal=y_pred_mal[k-1]\\n     if num_mal==1 :\\n       nombre_bien=i\\n       nombre_mal=\"gamma\"\\n       prob=round((1-y_percent[k-1])*100,4)\\n     elif num_mal==0:\\n       nombre_bien=\"gamma\"\\n       nombre_mal=i\\n       prob=round((y_percent[k-1])*100,4)\\n     plt.title(\"Es un \"+nombre_bien+\" pero \\n ha dicho con un \"+str(prob)+\"% \\n\"+\"que era un \"+nombre_mal, fontsize=14)\\n     plt.imshow(errores[k-1])\\n   plt.tight_layout()\\n   plt.savefig(direct_hist+\"errores_\"+i+\".png\")\\n   plt.close()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_NKvKy-IYLU",
        "outputId": "4361f627-ab95-4747-d7a8-8fdb58d7f7c3"
      },
      "source": [
        "#para el proton lo hacemos en dos\n",
        "npy_dir=\"/content/drive/MyDrive/TFG arturo/npy archivos\"\n",
        "direct_hist=\"/content/drive/MyDrive/TFG arturo/stats_1/clasif_CNN_1/\"\n",
        "#lo mejor va a ser tener cargadas unas cuantas de gamma y darselo conforme lo requiera \n",
        " \n",
        "nombre_aux=\"gamma_\"+str(1)+\".npy\"\n",
        "datos_gamma=np.load(os.path.join(npy_dir,nombre_aux))\n",
        "tama√±o_un_gamma=datos_gamma.nbytes #os.stat(\"/content/drive/MyDrive/TFG arturo/npy archivos/gamma_1.npy\").st_size\n",
        "elementos=[\"proton\"]\n",
        " \n",
        "for j in range(6,10):\n",
        "  nombre_aux=\"gamma_\"+str(j)+\".npy\"\n",
        "  load_aux=np.load(os.path.join(npy_dir,nombre_aux))\n",
        "  datos_gamma=np.concatenate((datos_gamma,load_aux),axis=0)\n",
        " \n",
        " \n",
        "for i in elementos:\n",
        "  #primero cargamos los datos necesarios\n",
        " \n",
        "  datos_elemento=np.load(os.path.join(npy_dir,i+\".npy\"))\n",
        "  \n",
        "  len_ds=int(round(datos_elemento.shape[0]/2))\n",
        "  datos_elemento=datos_elemento[len_ds:]\n",
        "  #vamos a cargar de datos los mismo pero de gammas\n",
        "  datos_gamma_aux=datos_gamma[:len_ds].copy()\n",
        "  datos=np.concatenate((datos_elemento,datos_gamma_aux),axis=0)\n",
        " \n",
        "  labels0=np.zeros(len_ds)\n",
        "  labels1=np.ones(len_ds)\n",
        "  labels=np.concatenate((labels0,labels1),axis=0)\n",
        "  X_train_aux,X_test_aux,y_train,y_test=train_test_split(datos,labels,test_size=0.20, random_state=3)\n",
        "  X_train=np.expand_dims(X_train_aux,axis=-1)\n",
        "  X_test=np.expand_dims(X_test_aux,axis=-1)\n",
        "  data_train=tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(128)\n",
        "  data_train=data_train.prefetch(1)\n",
        "  data_test=tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(128)\n",
        "  data_test=data_test.prefetch(1) \n",
        "\n",
        "  model=tf.keras.models.load_model(direct_hist+\"modelo_\"+i+\".h5\")\n",
        "  history=model.fit(data_train, epochs=60,batch_size=128, validation_data=data_test)  \n",
        "\n",
        "  #vamos a borrar lo que no necesitamos\n",
        "  del data_train, datos_elemento, datos, datos_gamma_aux\n",
        "  del X_train, X_train_aux, y_train\n",
        "  model.save(direct_hist+\"modelo_\"+i+\"_2_final.h5\")\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('2¬∫ Model loss, gamma vs.'+i)\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper right')\n",
        "  plt.savefig(direct_hist+\"2_loss_train_gamma_vs_\"+i+\".png\")\n",
        "  plt.close()\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('2¬∫ Model accuracy, gamma vs.'+i)\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='lower right')\n",
        "  plt.savefig(direct_hist+\"2_acc_train_gamma_vs_\"+i+\".png\")\n",
        "  plt.close()\n",
        " \n",
        "  val_loss, val_acc = model.evaluate(data_test)\n",
        "  y_pred = model.predict_classes(X_test)\n",
        "  conf_aux=confusion_matrix(y_test, y_pred)\n",
        "  report_aux=classification_report(y_test, y_pred)\n",
        "\n",
        "  a=y_test.shape[0]\n",
        "  b=y_test[y_test != y_pred]\n",
        " \n",
        "  plt.figure(figsize=(15,15))\n",
        "  y_percent=model.predict(X_test)\n",
        "  y_percent=y_percent[:,0][y_test!=y_pred]\n",
        "  y_pred_mal=y_pred[y_test!=y_pred]\n",
        "  errores=X_test_aux[y_test!=y_pred]\n",
        "  with open(direct_hist+i+\"_report_2.txt\",\"w\") as report:\n",
        "    report.write(\"val_loss: \"+str(val_loss)+\"\\t\"+\"val_acc: \"+str(val_acc)+\"\\n\")\n",
        "    report.write(str(conf_aux)+\"\\n\"+str(report_aux)+\"\\n\")\n",
        "    report.write(\"test_acc bien: \" +str(1-b.shape[0]/a))\n",
        "    report.close()\n",
        "  if (errores.shape[0]>=16):\n",
        "    rango=range(1,17)\n",
        "    asd=4 \n",
        "    asd2=4\n",
        "    abortar=False\n",
        " \n",
        "  elif errores.shape[0]>=9:\n",
        "    rango=range(1,10)\n",
        "    asd=3\n",
        "    asd2=3\n",
        "    abortar=False\n",
        " \n",
        "  elif errores.shape[0]>=4:\n",
        "    rango=range(1,5)\n",
        "    asd=2\n",
        "    asd2=2\n",
        "    abortar=False\n",
        "  elif errores.shape[0]>=2:\n",
        "    rango=range(1,3)\n",
        "    asd=2     \n",
        "    asd2=1\n",
        "    abortar=False\n",
        "  elif errores.shape[0]>=1:\n",
        "    rango=range(1,2)\n",
        "    asd=1\n",
        "    asd2=1\n",
        "    abortar=False \n",
        "  else:\n",
        "    abortar=True \n",
        "  if not abortar:\n",
        "    for k in rango:\n",
        "      plt.subplot(asd,asd2,k)\n",
        "      num_mal=y_pred_mal[k-1]\n",
        "      if num_mal==1 :\n",
        "        nombre_bien=i\n",
        "        nombre_mal=\"gamma\"\n",
        "        prob=round((1-y_percent[k-1])*100,4)\n",
        "      elif num_mal==0:\n",
        "        nombre_bien=\"gamma\"\n",
        "        nombre_mal=i\n",
        "        prob=round((y_percent[k-1])*100,4)\n",
        "      plt.title(\"Es un \"+nombre_bien+\" pero \\n ha dicho con un \"+str(prob)+\"% \\n\"+\"que era un \"+nombre_mal, fontsize=14)\n",
        "      plt.imshow(errores[k-1])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(direct_hist+\"2_errores_\"+i+\".png\")\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "  2/170 [..............................] - ETA: 5s - loss: 8.2917e-04 - accuracy: 0.5195WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0232s vs `on_train_batch_end` time: 0.0350s). Check your callbacks.\n",
            "170/170 [==============================] - 11s 65ms/step - loss: 0.0142 - accuracy: 0.4974 - val_loss: 0.0076 - val_accuracy: 0.5066\n",
            "Epoch 2/60\n",
            "170/170 [==============================] - 10s 61ms/step - loss: 0.0117 - accuracy: 0.4976 - val_loss: 0.0061 - val_accuracy: 0.5066\n",
            "Epoch 3/60\n",
            "170/170 [==============================] - 10s 62ms/step - loss: 0.0108 - accuracy: 0.4976 - val_loss: 0.0047 - val_accuracy: 0.5057\n",
            "Epoch 4/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0086 - accuracy: 0.4981 - val_loss: 0.0092 - val_accuracy: 0.5044\n",
            "Epoch 5/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0210 - accuracy: 0.4974 - val_loss: 0.0030 - val_accuracy: 0.5048\n",
            "Epoch 6/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0064 - accuracy: 0.4986 - val_loss: 0.0022 - val_accuracy: 0.5057\n",
            "Epoch 7/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0122 - accuracy: 0.4987 - val_loss: 0.0031 - val_accuracy: 0.5059\n",
            "Epoch 8/60\n",
            "170/170 [==============================] - 11s 64ms/step - loss: 0.0093 - accuracy: 0.4983 - val_loss: 0.0040 - val_accuracy: 0.5046\n",
            "Epoch 9/60\n",
            "170/170 [==============================] - 11s 64ms/step - loss: 0.0057 - accuracy: 0.4982 - val_loss: 0.0055 - val_accuracy: 0.5077\n",
            "Epoch 10/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0080 - accuracy: 0.4980 - val_loss: 0.0072 - val_accuracy: 0.5052\n",
            "Epoch 11/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0077 - accuracy: 0.4979 - val_loss: 0.0031 - val_accuracy: 0.5053\n",
            "Epoch 12/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0063 - accuracy: 0.4980 - val_loss: 0.0028 - val_accuracy: 0.5061\n",
            "Epoch 13/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0038 - accuracy: 0.4981 - val_loss: 0.0088 - val_accuracy: 0.5055\n",
            "Epoch 14/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0057 - accuracy: 0.4983 - val_loss: 0.0055 - val_accuracy: 0.5059\n",
            "Epoch 15/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0082 - accuracy: 0.4977 - val_loss: 0.0049 - val_accuracy: 0.5072\n",
            "Epoch 16/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0064 - accuracy: 0.4982 - val_loss: 0.0048 - val_accuracy: 0.5059\n",
            "Epoch 17/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0066 - accuracy: 0.4977 - val_loss: 0.0049 - val_accuracy: 0.5055\n",
            "Epoch 18/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0071 - accuracy: 0.4980 - val_loss: 0.0034 - val_accuracy: 0.5059\n",
            "Epoch 19/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0073 - accuracy: 0.4985 - val_loss: 0.0120 - val_accuracy: 0.5050\n",
            "Epoch 20/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0047 - accuracy: 0.4983 - val_loss: 0.0078 - val_accuracy: 0.5059\n",
            "Epoch 21/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0046 - accuracy: 0.4981 - val_loss: 0.0113 - val_accuracy: 0.5053\n",
            "Epoch 22/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0082 - accuracy: 0.4983 - val_loss: 0.0034 - val_accuracy: 0.5053\n",
            "Epoch 23/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0031 - accuracy: 0.4979 - val_loss: 0.0174 - val_accuracy: 0.5052\n",
            "Epoch 24/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0115 - accuracy: 0.4976 - val_loss: 0.0069 - val_accuracy: 0.5055\n",
            "Epoch 25/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0069 - accuracy: 0.4983 - val_loss: 0.0057 - val_accuracy: 0.5059\n",
            "Epoch 26/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0077 - accuracy: 0.4981 - val_loss: 0.0055 - val_accuracy: 0.5055\n",
            "Epoch 27/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0049 - accuracy: 0.4982 - val_loss: 0.0049 - val_accuracy: 0.5055\n",
            "Epoch 28/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0053 - accuracy: 0.4981 - val_loss: 0.0120 - val_accuracy: 0.5050\n",
            "Epoch 29/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0050 - accuracy: 0.4984 - val_loss: 0.0146 - val_accuracy: 0.5055\n",
            "Epoch 30/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0080 - accuracy: 0.4980 - val_loss: 0.0166 - val_accuracy: 0.5063\n",
            "Epoch 31/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0063 - accuracy: 0.4983 - val_loss: 0.0066 - val_accuracy: 0.5048\n",
            "Epoch 32/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0039 - accuracy: 0.4983 - val_loss: 0.0056 - val_accuracy: 0.5057\n",
            "Epoch 33/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0044 - accuracy: 0.4982 - val_loss: 0.0110 - val_accuracy: 0.5053\n",
            "Epoch 34/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0065 - accuracy: 0.4983 - val_loss: 0.0089 - val_accuracy: 0.5053\n",
            "Epoch 35/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0082 - accuracy: 0.4981 - val_loss: 0.0083 - val_accuracy: 0.5053\n",
            "Epoch 36/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0099 - accuracy: 0.4981 - val_loss: 0.0054 - val_accuracy: 0.5055\n",
            "Epoch 37/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0041 - accuracy: 0.4983 - val_loss: 0.0025 - val_accuracy: 0.5055\n",
            "Epoch 38/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0038 - accuracy: 0.4982 - val_loss: 0.0114 - val_accuracy: 0.5052\n",
            "Epoch 39/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0074 - accuracy: 0.4980 - val_loss: 0.0102 - val_accuracy: 0.5046\n",
            "Epoch 40/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0042 - accuracy: 0.4982 - val_loss: 0.0073 - val_accuracy: 0.5050\n",
            "Epoch 41/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0034 - accuracy: 0.4985 - val_loss: 0.0092 - val_accuracy: 0.5052\n",
            "Epoch 42/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0037 - accuracy: 0.4982 - val_loss: 0.0066 - val_accuracy: 0.5055\n",
            "Epoch 43/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0033 - accuracy: 0.4982 - val_loss: 0.0252 - val_accuracy: 0.5050\n",
            "Epoch 44/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0063 - accuracy: 0.4988 - val_loss: 0.0142 - val_accuracy: 0.5133\n",
            "Epoch 45/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0040 - accuracy: 0.4987 - val_loss: 0.0088 - val_accuracy: 0.5055\n",
            "Epoch 46/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0061 - accuracy: 0.4985 - val_loss: 0.0056 - val_accuracy: 0.5061\n",
            "Epoch 47/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0041 - accuracy: 0.4985 - val_loss: 0.0069 - val_accuracy: 0.5053\n",
            "Epoch 48/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0063 - accuracy: 0.4984 - val_loss: 0.0054 - val_accuracy: 0.5055\n",
            "Epoch 49/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0044 - accuracy: 0.4984 - val_loss: 0.0041 - val_accuracy: 0.5055\n",
            "Epoch 50/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0034 - accuracy: 0.4984 - val_loss: 0.0029 - val_accuracy: 0.5055\n",
            "Epoch 51/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0051 - accuracy: 0.4980 - val_loss: 0.0083 - val_accuracy: 0.5053\n",
            "Epoch 52/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0030 - accuracy: 0.4985 - val_loss: 0.0111 - val_accuracy: 0.5055\n",
            "Epoch 53/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0052 - accuracy: 0.4983 - val_loss: 0.0062 - val_accuracy: 0.5053\n",
            "Epoch 54/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0036 - accuracy: 0.4983 - val_loss: 0.0069 - val_accuracy: 0.5055\n",
            "Epoch 55/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0048 - accuracy: 0.4986 - val_loss: 0.0038 - val_accuracy: 0.5055\n",
            "Epoch 56/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0035 - accuracy: 0.4983 - val_loss: 0.0079 - val_accuracy: 0.5053\n",
            "Epoch 57/60\n",
            "170/170 [==============================] - 11s 63ms/step - loss: 0.0042 - accuracy: 0.4982 - val_loss: 0.0122 - val_accuracy: 0.5053\n",
            "Epoch 58/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0087 - accuracy: 0.4986 - val_loss: 0.0048 - val_accuracy: 0.5052\n",
            "Epoch 59/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0045 - accuracy: 0.4984 - val_loss: 0.0042 - val_accuracy: 0.5053\n",
            "Epoch 60/60\n",
            "170/170 [==============================] - 11s 62ms/step - loss: 0.0165 - accuracy: 0.4981 - val_loss: 0.0033 - val_accuracy: 0.5055\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.0033 - accuracy: 0.5055\n",
            "WARNING:tensorflow:From <ipython-input-3-cbd68e335800>:66: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtBHOXVHRZYV",
        "outputId": "c6440bd0-bdc9-4a36-d5c0-4a1f0526d126"
      },
      "source": [
        "1-0.00230117820"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9976988218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "jTBRQP74NoQJ",
        "outputId": "e0a0a2cb-130a-4010-f264-95c3a52f829f"
      },
      "source": [
        "plt.figure(figsize=(14,14))\n",
        "plt.subplot(3,3,2)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "ax=plt.gca()\n",
        "plt.plot()\n",
        "#plt.text(-2.4,0.2,\"Suma de todas las intensidades \\n en cada pixel.\")\n",
        "texts=[\"Suma de todas las intensidades \\n superiores al percentil 75.\",\"Suma de todas las intensidades \\n superiores al percentil 85.\",\"Suma de todas las intensidades \\n superiores al percentil 99,9.\",\n",
        "       \"N√∫mero de las incidencias de fotones \\n superiores al percentil 75 de energ√≠as.\",\"N√∫mero de las incidencias de fotones \\n superiores al percentil 85 de energ√≠as.\",\"N√∫mero de las incidencias de fotones \\n superiores al percentil 99,9 de energ√≠as.\"]\n",
        "for i in range(4,10):\n",
        "  plt.subplot(3,3,i)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.savefig(\"blank_im.png\")\n",
        "  #plt.text(-0.4,0.2,texts[i-4])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x1008 with 7 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB_TTLDdNyJN",
        "outputId": "665a66d0-7337-447e-8767-d6893bb85876"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 55, 93, 32)        832       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 55, 93, 32)        25632     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 27, 46, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 27, 46, 64)        51264     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 27, 46, 64)        102464    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 19136)             0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 19136)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 20)                382740    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 42        \n",
            "=================================================================\n",
            "Total params: 563,394\n",
            "Trainable params: 563,394\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO_YXJjaSioZ",
        "outputId": "52b68a80-9cf6-4605-e495-75dbd084beab"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 55, 93, 1),\n",
              "    'dtype': 'float64',\n",
              "    'name': 'conv2d_input',\n",
              "    'ragged': False,\n",
              "    'sparse': False}},\n",
              "  {'class_name': 'Conv2D',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'data_format': 'channels_last',\n",
              "    'dilation_rate': (1, 1),\n",
              "    'dtype': 'float32',\n",
              "    'filters': 32,\n",
              "    'groups': 1,\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotNormal',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'kernel_size': (5, 5),\n",
              "    'name': 'conv2d',\n",
              "    'padding': 'same',\n",
              "    'strides': (1, 1),\n",
              "    'trainable': True,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Conv2D',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'data_format': 'channels_last',\n",
              "    'dilation_rate': (1, 1),\n",
              "    'dtype': 'float32',\n",
              "    'filters': 32,\n",
              "    'groups': 1,\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotNormal',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'kernel_size': (5, 5),\n",
              "    'name': 'conv2d_1',\n",
              "    'padding': 'same',\n",
              "    'strides': (1, 1),\n",
              "    'trainable': True,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'MaxPooling2D',\n",
              "   'config': {'data_format': 'channels_last',\n",
              "    'dtype': 'float32',\n",
              "    'name': 'max_pooling2d',\n",
              "    'padding': 'valid',\n",
              "    'pool_size': (2, 2),\n",
              "    'strides': (2, 2),\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Conv2D',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'data_format': 'channels_last',\n",
              "    'dilation_rate': (1, 1),\n",
              "    'dtype': 'float32',\n",
              "    'filters': 64,\n",
              "    'groups': 1,\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotNormal',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'kernel_size': (5, 5),\n",
              "    'name': 'conv2d_2',\n",
              "    'padding': 'same',\n",
              "    'strides': (1, 1),\n",
              "    'trainable': True,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Conv2D',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'data_format': 'channels_last',\n",
              "    'dilation_rate': (1, 1),\n",
              "    'dtype': 'float32',\n",
              "    'filters': 64,\n",
              "    'groups': 1,\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotNormal',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'kernel_size': (5, 5),\n",
              "    'name': 'conv2d_3',\n",
              "    'padding': 'same',\n",
              "    'strides': (1, 1),\n",
              "    'trainable': True,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'MaxPooling2D',\n",
              "   'config': {'data_format': 'channels_last',\n",
              "    'dtype': 'float32',\n",
              "    'name': 'max_pooling2d_1',\n",
              "    'padding': 'valid',\n",
              "    'pool_size': (2, 2),\n",
              "    'strides': (2, 2),\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Flatten',\n",
              "   'config': {'data_format': 'channels_last',\n",
              "    'dtype': 'float32',\n",
              "    'name': 'flatten',\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.4,\n",
              "    'seed': None,\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotNormal',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense',\n",
              "    'trainable': True,\n",
              "    'units': 20,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout_1',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.4,\n",
              "    'seed': None,\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotNormal',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_1',\n",
              "    'trainable': True,\n",
              "    'units': 20,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout_2',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.4,\n",
              "    'seed': None,\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'softmax',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_2',\n",
              "    'trainable': True,\n",
              "    'units': 2,\n",
              "    'use_bias': True}}],\n",
              " 'name': 'sequential'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKk849eSSxsq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}